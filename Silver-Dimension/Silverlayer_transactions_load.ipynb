{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6e8473-5978-421e-88e5-f358a5a5963b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01-Config/02-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ed6ff1-8bd8-42b9-9f40-0ee97d937bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ECOMMERCE_DB.SILVER_DB.TRANSACTIONS (\n",
    "    transaction_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    quantity INT,\n",
    "    total_amount DOUBLE,\n",
    "    transaction_date DATE,\n",
    "    payment_method STRING,\n",
    "    store_type STRING,\n",
    "    order_status STRING,\n",
    "    last_updated TIMESTAMP\n",
    ")\"\"\"\n",
    "spark._jvm.net.snowflake.spark.snowflake.Utils.runQuery(snowflake_config, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9d5dd9-9c5e-4d04-a1d7-a198be42e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the last processed timestamp from silver layer\n",
    "customer_query = \"SELECT MAX(last_updated) as last_processed FROM ECOMMERCE_DB.SILVER_DB.TRANSACTIONS\"\n",
    "last_processed_df = spark.read.format(\"snowflake\") \\\n",
    "    .options(**snowflake_config) \\\n",
    "    .option(\"query\", customer_query) \\\n",
    "    .load()\n",
    "# Display the schema to check the column names\n",
    "last_processed_df.printSchema()\n",
    "\n",
    "# Check if the DataFrame is empty\n",
    "if last_processed_df.count() == 0:\n",
    "    last_processed_timestamp = \"1900-01-01T00:00:00.000+00:00\"\n",
    "else:\n",
    "    last_processed_row = last_processed_df.collect()[0]\n",
    "    last_processed_timestamp = last_processed_row['LAST_PROCESSED'] if 'LAST_PROCESSED' in last_processed_row else None\n",
    "    if last_processed_timestamp is None:\n",
    "        last_processed_timestamp = \"1900-01-01T00:00:00.000+00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8931e2f2-df2f-481b-a08b-cfc657ac4b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ECOMMERCE_DB.BRONZE.transaction c \n",
    "WHERE c.ingestion_timestamp > '{last_processed_timestamp}'\n",
    "\"\"\"\n",
    "transaction_view = spark.read.format(\"snowflake\") \\\n",
    "    .options(**snowflake_config) \\\n",
    "    .option(\"query\", query) \\\n",
    "    .load() \n",
    "\n",
    "#customer_view.createOrReplaceTempView(\"bronze_incremental\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76de3828-6983-432e-b3c9-a3b7b45534be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transaction_view.createOrReplaceTempView(\"transaction_incremental\")\n",
    "spark.sql(\"select * from transaction_incremental\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f4a235-86a4-4542-83a9-68e80fa13c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Transformations:\n",
    "   - Quantity and total_amount normalization (setting negative values to 0)\n",
    "   - Date casting to ensure consistent date format\n",
    "   - Order status derivation based on quantity and total_amount\n",
    "\n",
    "Data Quality Checks: We filter out records with null transaction dates, customer IDs, or product IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210d936e-80a9-4def-ae70-c79941f3cda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW silver_incremental_orders AS\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CASE \n",
    "        WHEN quantity < 0 THEN 0 \n",
    "        ELSE quantity \n",
    "    END AS quantity,\n",
    "    CASE \n",
    "        WHEN total_amount < 0 THEN 0 \n",
    "        ELSE total_amount \n",
    "    END AS total_amount,\n",
    "    CAST(transaction_date AS DATE) AS transaction_date,\n",
    "    payment_method,\n",
    "    store_type,\n",
    "    CASE\n",
    "        WHEN quantity = 0 OR total_amount = 0 THEN 'Cancelled'\n",
    "        ELSE 'Completed'\n",
    "    END AS order_status,\n",
    "    CURRENT_TIMESTAMP() AS last_updated\n",
    "FROM transaction_incremental\n",
    "WHERE transaction_date IS NOT NULL\n",
    "  AND customer_id IS NOT NULL\n",
    "  AND product_id IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d23a4d-487c-4edc-8e8a-d54161f4abbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from silver_incremental_orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0c1686-6050-42b6-a14f-99af0f71b2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_query = \"SELECT * FROM ECOMMERCE_DB.SILVER_DB.TRANSACTIONS\"\n",
    "target_df = spark.read.format(\"snowflake\") \\\n",
    "    .options(**snowflake_config) \\\n",
    "    .option(\"query\", customer_query) \\\n",
    "    .load()\n",
    "\n",
    "display(target_df)\n",
    "\n",
    "source_cust = spark.sql(\"select * from silver_incremental_orders\")\n",
    "upd_cust = target_df.join(source_cust, \"transaction_id\", \"inner\").select(source_cust[\"*\"])\n",
    "new_cust = target_df.join(source_cust, \"transaction_id\", \"right\").select(source_cust[\"*\"])\n",
    "\n",
    "display(upd_cust)\n",
    "display(new_cust)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc588af-42a2-4c86-9fe4-f6cda6d2348a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#wite data staging tables to silver schema in snowflake\n",
    "\n",
    "new_cust.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**snowflake_config) \\\n",
    "    .option(\"dbtable\", 'transactions') \\\n",
    "    .option(\"sfDatabase\", 'ecommerce_db') \\\n",
    "    .option(\"sfSchema\", 'silver_db') \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "upd_cust.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**snowflake_config) \\\n",
    "    .option(\"dbtable\", 'transactions') \\\n",
    "    .option(\"sfDatabase\", 'ecommerce_db') \\\n",
    "    .option(\"sfSchema\", 'silver_db') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 432430608335516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silverlayer_transactions_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
